{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "22e6d18b-3b80-4520-a925-1b287cadc63a",
      "metadata": {
        "id": "22e6d18b-3b80-4520-a925-1b287cadc63a"
      },
      "source": [
        "# Word Embeddings\n",
        "In this notebook, we will: 1.train word embeddings using word2vec, import GloVe\n",
        "\n",
        "1.   Train word embeddings using word2vec\n",
        "2.   Import GloVe embeddings\n",
        "3.   Use these embeddings as input for classification using again the CoronaNLP\n",
        "     dataset\n",
        "4.   EXTRA: Implement the Skip-Gram model from scratch using pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a195a8d",
      "metadata": {
        "id": "2a195a8d"
      },
      "source": [
        "# 1. Training Word Embeddings with Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a31ce9b-5e2d-4063-8c06-0816c3d718be",
      "metadata": {
        "id": "0a31ce9b-5e2d-4063-8c06-0816c3d718be"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "source": [
        "#after installing restart the kernel\n",
        "#!pip install gensim"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9fLU43dps4zh"
      },
      "id": "9fLU43dps4zh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "dtcI_gC4s1S1"
      },
      "id": "dtcI_gC4s1S1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus: list of tokenized sentences\n",
        "corpus = [\n",
        "    [\"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"dog\", \"barked\", \"at\", \"the\", \"cat\"],\n",
        "    [\"bird\", \"flew\", \"over\", \"the\", \"house\"],\n",
        "    [\"cat\", \"and\", \"dog\", \"are\", \"friends\"],\n",
        "]"
      ],
      "metadata": {
        "id": "-b1fXRNDtUnz"
      },
      "id": "-b1fXRNDtUnz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure corpus is a list of lists\n",
        "#if isinstance(corpus[0], str):\n",
        "   # corpus = [sentence.split() for sentence in corpus]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=corpus,\n",
        "    vector_size=5,    # size of the embedding vectors\n",
        "    window=2,         # context window size\n",
        "    min_count=1,      # minimum word frequency to include\n",
        "    sg=1              # 1 for skip-gram; 0 for CBOW\n",
        ")"
      ],
      "metadata": {
        "id": "rxYNxFlntbk_"
      },
      "id": "rxYNxFlntbk_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "# model.save(\"word2vec.model\")\n",
        "\n",
        "# Load model (example)\n",
        "# model = Word2Vec.load(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "smQqxEB-tlrw"
      },
      "id": "smQqxEB-tlrw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: access embedding vector\n",
        "cat_vector = model.wv[\"cat\"]\n",
        "print(\"Embedding vector for 'cat':\", cat_vector)"
      ],
      "metadata": {
        "id": "UFo--68R1Rz8"
      },
      "id": "UFo--68R1Rz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: find most similar words\n",
        "print(\"Most similar to 'cat':\", model.wv.most_similar(\"cat\"))"
      ],
      "metadata": {
        "id": "4NuZ9fTPtnSa"
      },
      "id": "4NuZ9fTPtnSa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: find odd word out\n",
        "print(\"Odd one out among ['cat', 'dog', 'bird', 'house']:\", model.wv.doesnt_match([\"cat\", \"dog\", \"bird\", \"house\"]))"
      ],
      "metadata": {
        "id": "xQ4K1rMvwWhs"
      },
      "id": "xQ4K1rMvwWhs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: similarity between two words\n",
        "print(\"Similarity between 'cat' and 'dog':\", model.wv.similarity(\"cat\", \"dog\"))"
      ],
      "metadata": {
        "id": "EYm2z_XHwagM"
      },
      "id": "EYm2z_XHwagM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ea796763-8393-4c66-8d88-40decec974ed",
      "metadata": {
        "tags": [],
        "id": "ea796763-8393-4c66-8d88-40decec974ed"
      },
      "source": [
        "### Word Embeddings Visualization\n",
        "\n",
        "Go to https://projector.tensorflow.org/ and visualize Word2Vec embeddings.\n",
        "\n",
        "Original Word2Vec repository: https://code.google.com/archive/p/word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KankEDXXsr8q"
      },
      "source": [
        "# 2. Exploring Word Vectors with GloVe\n",
        "\n",
        "As we have seen, the Word2vec algorithms (such as Skip-Gram) predicts words in a context (e.g. what is the most likely word to appear in \"the cat ? the mouse\"). GloVe vectors are based on global counts across the corpus.  \n",
        "\n",
        "The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors â€” see [How is GloVe different from word2vec?](https://www.quora.com/How-is-GloVe-different-from-word2vec) and [Intuitive Guide to Understanding GloVe Embeddings](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)  for some better explanations.\n",
        "\n",
        "Multiple sets of pre-trained GloVe vectors are easily available for [download](https://nlp.stanford.edu/projects/glove/), so that's what we'll use here.\n",
        "\n",
        "Part of this section is taken from [practical-pytorch tutorials](https://github.com/spro/practical-pytorch/blob/master/glove-word-vectors/glove-word-vectors.ipynb)"
      ],
      "id": "KankEDXXsr8q"
    },
    {
      "cell_type": "markdown",
      "id": "a0fa8fca",
      "metadata": {
        "id": "a0fa8fca"
      },
      "source": [
        "### Loading word vectors\n",
        "\n",
        "Gensim includes functions to download embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f916ee8",
      "metadata": {
        "id": "3f916ee8"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "956a59e3",
      "metadata": {
        "id": "956a59e3"
      },
      "outputs": [],
      "source": [
        "print(list(gensim.downloader.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dim=100"
      ],
      "metadata": {
        "id": "DRpQAYTJ5JFx"
      },
      "id": "DRpQAYTJ5JFx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#glove-twitter-25 has embeddings size 25, glove-twitter-100 has embeddings size 100, etc.\n",
        "glove_model = gensim.downloader.load(f'glove-twitter-{dim}')"
      ],
      "metadata": {
        "id": "2I6JOpPYwzJ5"
      },
      "id": "2I6JOpPYwzJ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.get_vector(\"lol\")"
      ],
      "metadata": {
        "id": "KtEWPTKPw6s3"
      },
      "id": "KtEWPTKPw6s3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most similar words to a given word\n",
        "print(\"Example - Most similar to 'computer':\", glove_model.most_similar('computer')[:3])"
      ],
      "metadata": {
        "id": "IUWBCXgExmuJ"
      },
      "id": "IUWBCXgExmuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the similarity between two words\n",
        "print(\"Example - Similarity between 'computer' and 'laptop':\", glove_model.similarity('computer', 'laptop'))"
      ],
      "metadata": {
        "id": "-IO9QWjzxmrk"
      },
      "id": "-IO9QWjzxmrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA for GloVe embeddings\n",
        "glove_matrix = np.array([glove_model[word] for word in glove_model.index_to_key[:100]])\n",
        "pca = PCA(n_components=2)\n",
        "glove_embeddings_2d = pca.fit_transform(glove_matrix)"
      ],
      "metadata": {
        "id": "7EbMXNyvxmnU"
      },
      "id": "7EbMXNyvxmnU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "plt.scatter(glove_embeddings_2d[:, 0], glove_embeddings_2d[:, 1], alpha=0.6)\n",
        "selected_words = glove_model.index_to_key[:100]\n",
        "for i, word in enumerate(selected_words):\n",
        "    plt.annotate(word, (glove_embeddings_2d[i, 0], glove_embeddings_2d[i, 1]), fontsize=8, alpha=0.7)\n",
        "plt.title(f'Word Embeddings (First 100 Words)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8AIjd8jkz5e-"
      },
      "id": "8AIjd8jkz5e-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "434e44f8",
      "metadata": {
        "id": "434e44f8"
      },
      "source": [
        "### Word analogies with vector arithmetic\n",
        "The most interesting feature of a well-trained word vector space is that certain semantic relationships (beyond just closeness of words) can be captured with regular vector arithmetic.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "\n",
        "(image borrowed from https://jalammar.github.io/illustrated-word2vec/)\n",
        "\n",
        "Read [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = glove_model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
        "print(\"\\nWord Analogy Example (king - man + woman):\", result[:5])"
      ],
      "metadata": {
        "id": "5MlgYKuK1l-7"
      },
      "id": "5MlgYKuK1l-7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ade2388f",
      "metadata": {
        "id": "ade2388f"
      },
      "outputs": [],
      "source": [
        "def analogy(word1, word2, word3, model=glove_model, topn=3):\n",
        "    try:\n",
        "        result = model.most_similar(positive=[word2, word3], negative=[word1], topn=topn)\n",
        "        print(f\"\\nAnalogy ({word1} -> {word2} like {word3} -> ?):\")\n",
        "        for word, similarity in result:\n",
        "            print(f\"- {word}: {similarity:.4f}\")\n",
        "    except KeyError as e:\n",
        "        print(f\"Word not in vocabulary: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c19809",
      "metadata": {
        "id": "e3c19809"
      },
      "outputs": [],
      "source": [
        "analogy('king', 'man', 'queen')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e3586f",
      "metadata": {
        "id": "48e3586f"
      },
      "source": [
        "Now let's explore the word space and see what stereotypes we can uncover:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52b62733",
      "metadata": {
        "id": "52b62733"
      },
      "outputs": [],
      "source": [
        "analogy('man', 'actor', 'woman')\n",
        "analogy('cat', 'kitten', 'dog')\n",
        "analogy('dog', 'puppy', 'cat')\n",
        "analogy('russia', 'moscow', 'france')\n",
        "analogy('obama', 'president', 'trump')\n",
        "analogy('rich', 'mansion', 'poor')\n",
        "analogy('elvis', 'rock', 'eminem')\n",
        "analogy('paper', 'newspaper', 'screen')\n",
        "analogy('monet', 'paint', 'michelangelo')\n",
        "analogy('beer', 'barley', 'wine')\n",
        "analogy('earth', 'moon', 'sun')\n",
        "analogy('house', 'roof', 'castle')\n",
        "analogy('building', 'architect', 'software')\n",
        "analogy('good', 'heaven', 'bad')\n",
        "analogy('jordan', 'basketball', 'ronaldo')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984d0117",
      "metadata": {
        "id": "984d0117"
      },
      "source": [
        "# 3. Training and Classification with Word Embeddings\n",
        "### Now we will see how we can apply word embeddings to feature engineer our corpus and classify the sentiment of **tweets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9588d338",
      "metadata": {
        "scrolled": true,
        "id": "9588d338"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Corona_NLP.csv\", encoding='latin-1')\n",
        "pd.options.display.max_colwidth = 500\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e169d75",
      "metadata": {
        "id": "0e169d75"
      },
      "outputs": [],
      "source": [
        "df  = df[['OriginalTweet', 'Sentiment']].head(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9aa336e",
      "metadata": {
        "id": "e9aa336e"
      },
      "outputs": [],
      "source": [
        "set(df['Sentiment'].values)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['Sentiment']!=\"Neutral\"]"
      ],
      "metadata": {
        "id": "NvbcHyBE5_H9"
      },
      "id": "NvbcHyBE5_H9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d25c50d",
      "metadata": {
        "id": "9d25c50d"
      },
      "outputs": [],
      "source": [
        "df['LabelSentiment'] = df['Sentiment'].apply(lambda x: 1 if x in ['Extremely Positive', 'Positive'] else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b27b226c",
      "metadata": {
        "id": "b27b226c"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208d0731",
      "metadata": {
        "id": "208d0731"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['OriginalTweet'], df['LabelSentiment'], test_size=0.20, random_state=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d09f121",
      "metadata": {
        "id": "9d09f121"
      },
      "outputs": [],
      "source": [
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.hist()"
      ],
      "metadata": {
        "id": "k8VoMWIC6LKd"
      },
      "id": "k8VoMWIC6LKd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e08fc03a",
      "metadata": {
        "id": "e08fc03a"
      },
      "source": [
        "### Clean text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b58f362b",
      "metadata": {
        "id": "b58f362b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer('english')\n",
        "lemma = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2add9538",
      "metadata": {
        "id": "2add9538"
      },
      "outputs": [],
      "source": [
        "def clean(text_list):\n",
        "\n",
        "    updates = []\n",
        "\n",
        "    for j in tqdm(text_list):\n",
        "\n",
        "        text = j\n",
        "\n",
        "        #LOWERCASE TEXT\n",
        "        text = text.lower()\n",
        "\n",
        "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
        "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
        "\n",
        "        #REMOVE STOPWORDS\n",
        "        text = \" \".join([word for word in text.split() if word not in stop])\n",
        "\n",
        "        #Lemmatize\n",
        "        text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
        "\n",
        "        updates.append(text)\n",
        "\n",
        "    return updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ef4c42",
      "metadata": {
        "id": "49ef4c42"
      },
      "outputs": [],
      "source": [
        "X_train_clean = clean(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875aee92",
      "metadata": {
        "id": "875aee92"
      },
      "outputs": [],
      "source": [
        "X_test_clean = clean(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee2b33d7",
      "metadata": {
        "id": "ee2b33d7"
      },
      "source": [
        "### Define extracting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_clean[:10]"
      ],
      "metadata": {
        "id": "xzsJtkeY1Och"
      },
      "id": "xzsJtkeY1Och",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract sentence embeddings from X_train_clean by averaging word embeddings per sentence ---\n",
        "def average_embedding(text, model, dim):\n",
        "    words = text.split()\n",
        "    vectors = []\n",
        "    for word in words:\n",
        "        if word in model:\n",
        "            vectors.append(model[word])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)"
      ],
      "metadata": {
        "id": "93-CfnJU1Ywy"
      },
      "id": "93-CfnJU1Ywy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embeddings = np.array([average_embedding(text, glove_model, dim=dim) for text in X_train_clean])"
      ],
      "metadata": {
        "id": "MQGi90mH3Zpg"
      },
      "id": "MQGi90mH3Zpg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embeddings"
      ],
      "metadata": {
        "id": "n6RuaKi_3cOC"
      },
      "id": "n6RuaKi_3cOC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_embeddings.shape"
      ],
      "metadata": {
        "id": "JChiUSVv6YdQ"
      },
      "id": "JChiUSVv6YdQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_embeddings = np.array([average_embedding(text, glove_model, dim=dim) for text in X_test_clean])"
      ],
      "metadata": {
        "id": "ZaOIn01b3e8a"
      },
      "id": "ZaOIn01b3e8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_embeddings, y_train)"
      ],
      "metadata": {
        "id": "Yec7OTLa3mHM"
      },
      "id": "Yec7OTLa3mHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_pred = clf.predict(X_test_embeddings)"
      ],
      "metadata": {
        "id": "DwKk2U6j4Htt"
      },
      "id": "DwKk2U6j4Htt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "L0S6xXjc4LM8"
      },
      "id": "L0S6xXjc4LM8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "95099b42-d779-4cd0-9827-9e63641facd5",
      "metadata": {
        "id": "95099b42-d779-4cd0-9827-9e63641facd5"
      },
      "source": [
        "# 4. EXTRA: Implemening and training the Skip-Gram model from scratch\n",
        "\n",
        "\n",
        "![skip-gram.png](attachment:skip-gram.png)\n",
        "\n",
        "**NOTE:** This part of the notebook requires you to install pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01abe533-e703-44d2-b0f5-e569c35a9837",
      "metadata": {
        "id": "01abe533-e703-44d2-b0f5-e569c35a9837"
      },
      "outputs": [],
      "source": [
        "#!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "CRTha1IayoTy"
      },
      "id": "CRTha1IayoTy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6a81d951",
      "metadata": {
        "id": "6a81d951"
      },
      "source": [
        "Let's start with a simple corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "209fa495",
      "metadata": {
        "id": "209fa495"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    'he is a king',\n",
        "    'she is a queen',\n",
        "    'she is mad',\n",
        "    'she is in love',\n",
        "    'a mountain falls',\n",
        "    'paris is france capital',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbad87a1",
      "metadata": {
        "id": "fbad87a1"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus):\n",
        "    tokens = [x.split() for x in corpus]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2a5a78",
      "metadata": {
        "id": "ee2a5a78"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = tokenize_corpus(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be96da0",
      "metadata": {
        "id": "4be96da0"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6510dede",
      "metadata": {
        "id": "6510dede"
      },
      "outputs": [],
      "source": [
        "vocabulary = {word for doc in tokenized_corpus for word in doc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6231afff",
      "metadata": {
        "id": "6231afff"
      },
      "outputs": [],
      "source": [
        "vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71da2e65",
      "metadata": {
        "id": "71da2e65"
      },
      "outputs": [],
      "source": [
        "word2idx = {w:idx for (idx, w) in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e109335f",
      "metadata": {
        "id": "e109335f"
      },
      "outputs": [],
      "source": [
        "word2idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09506057",
      "metadata": {
        "id": "09506057"
      },
      "source": [
        "As you have seen in the theoretical lesson, we want to build pairs of words that appear within the same context.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0657ce2a",
      "metadata": {
        "id": "0657ce2a"
      },
      "outputs": [],
      "source": [
        "def build_training(tokenized_corpus, word2idx, window_size=2):\n",
        "    idx_pairs = []\n",
        "\n",
        "    # for each sentence\n",
        "    for sentence in tokenized_corpus:\n",
        "        indices = [word2idx[word] for word in sentence]\n",
        "        # for each word, treated as center word\n",
        "        for center_word_pos in range(len(indices)):\n",
        "            # for each window position\n",
        "            for w in range(-window_size, window_size + 1):\n",
        "                context_word_pos = center_word_pos + w\n",
        "                # make sure not jump out sentence\n",
        "                if  context_word_pos < 0 or \\\n",
        "                    context_word_pos >= len(indices) or \\\n",
        "                    center_word_pos == context_word_pos:\n",
        "                    continue\n",
        "                context_word_idx = indices[context_word_pos]\n",
        "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
        "    return np.array(idx_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a668181f",
      "metadata": {
        "id": "a668181f"
      },
      "outputs": [],
      "source": [
        "training_pairs = build_training(tokenized_corpus, word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac07ed06",
      "metadata": {
        "scrolled": true,
        "id": "ac07ed06"
      },
      "outputs": [],
      "source": [
        "training_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26d633d",
      "metadata": {
        "id": "e26d633d"
      },
      "outputs": [],
      "source": [
        "def get_onehot_vector(word_idx, vocabulary):\n",
        "    x = torch.zeros(len(vocabulary)).float()\n",
        "    x[word_idx] = 1.0\n",
        "    return x\n",
        "\n",
        "def Skip_Gram(training_pairs, vocabulary, embedding_dims=5, learning_rate=0.001, epochs=10):\n",
        "\n",
        "    torch.manual_seed(3)\n",
        "\n",
        "    W1 = torch.randn(embedding_dims, len(vocabulary), requires_grad=True).float()\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for epo in tqdm(range(epochs)):\n",
        "        loss_val = 0\n",
        "\n",
        "        for input_word, target in training_pairs:\n",
        "            x = get_onehot_vector(input_word, vocabulary).float()\n",
        "            y_true = torch.from_numpy(np.array([target])).long()\n",
        "\n",
        "            # Matrix multiplication to obtain the input word embedding\n",
        "            z1 = torch.matmul(W1, x)\n",
        "\n",
        "            # Matrix multiplication to obtain the z score for each word\n",
        "            z2 = torch.matmul(z1, W1)\n",
        "\n",
        "            # Apply Log and softmax functions\n",
        "            log_softmax = F.log_softmax(z2, dim=0)\n",
        "\n",
        "            # Compute the negative-log-likelihood loss\n",
        "            loss = F.nll_loss(log_softmax.view(1,-1), y_true)# .view -> Returns a tensor with the same data but with a different shape.\n",
        "            loss_val += loss.item()# -item -> Returns the value of this tensor as a standard Python number.\n",
        "\n",
        "            # Compute the gradient in function of the error\n",
        "            loss.backward()\n",
        "\n",
        "            # Update your embeddings\n",
        "            W1.data -= learning_rate * W1.grad.data\n",
        "\n",
        "            W1.grad.data.zero_()\n",
        "            # .grad -> This attribute is None by default and becomes a Tensor the first time a call to backward()\n",
        "            #computes gradients. The attribute will then contain the gradients computed and future\n",
        "            #calls to backward() will accumulate (add) gradients into it.\n",
        "\n",
        "        losses.append(loss_val/len(training_pairs))\n",
        "\n",
        "    return W1, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4d2861",
      "metadata": {
        "id": "aa4d2861"
      },
      "outputs": [],
      "source": [
        "W1, losses = Skip_Gram(training_pairs, word2idx, epochs=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aa02001",
      "metadata": {
        "id": "3aa02001"
      },
      "outputs": [],
      "source": [
        "def plot_loss(loss):\n",
        "    x_axis = [epoch+1 for epoch in range(len(loss))]\n",
        "    plt.plot(x_axis, loss, '-g', linewidth=1, label='Train')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8215923",
      "metadata": {
        "id": "c8215923"
      },
      "outputs": [],
      "source": [
        "plot_loss(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f11f95",
      "metadata": {
        "id": "80f11f95"
      },
      "source": [
        "### Final Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f675e0",
      "metadata": {
        "id": "d4f675e0"
      },
      "outputs": [],
      "source": [
        "W = torch.t(W1).clone().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b18382",
      "metadata": {
        "id": "04b18382"
      },
      "outputs": [],
      "source": [
        "W[word2idx[\"she\"]], W[word2idx[\"mad\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d28568",
      "metadata": {
        "id": "56d28568"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "euclidean_distances([W[word2idx[\"she\"]].numpy()], [W[word2idx[\"falls\"]].numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd08722b",
      "metadata": {
        "id": "fd08722b"
      },
      "outputs": [],
      "source": [
        "euclidean_distances([W[word2idx[\"she\"]].numpy()], [W[word2idx[\"mad\"]].numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5abc678c",
      "metadata": {
        "id": "5abc678c"
      },
      "source": [
        "As you can see from the previous example the vector representing \"she\" and the vector representing \"mad\" are closer then the vector representing \"she\" and \"falls\". This happens because \"she\" and \"falls\" never appear together inside the same context window."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2a195a8d",
        "KankEDXXsr8q",
        "984d0117",
        "95099b42-d779-4cd0-9827-9e63641facd5"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}